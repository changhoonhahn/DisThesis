\chapter*{Conclusion}\addcontentsline{toc}{chapter}{Conclusion}

\todo{
something that references the intro and answers it  \\
In this dissertation, we have tackled key methodological challenges 
in galaxy clustering analyses and galaxy evolution with robust treatment of systmeatics, innovative
approaches to inference, and improved models of the galaxy-halo 
connection.
} 


\todo{Summary of \Chap{fc}}

\todo{Summary of \Chap{abc}}

\todo{Summary of \Chap{galenv}} % galaxy-halo connection from an observational view

\todo{Summary of \Chap{galhalo}} % galaxy-haloc onnection from a data-driven LCDM view

Over the next decade future surveys, namely eBOSS and DESI, will expand the 
cosmic volumes probed with redshifts by an order of magnitude. They have the 
potential to measure the growth of structure and constrain cosmological 
parameters with unprecedented precision. The main challenges for realizing 
their full statistical power are methodological. The frameworks I present
in this dissertation -- robust treatment of systematics, innovative approaches 
to accurate inference, and improved models of the galaxy-halo connection -- can be 
extended to these future surveys and used to tackle key methodological 
challenges. 
%by robust treatment of systematics, accurate modeling, and higher order statistics.
%eBOSS and DESI will expand the cosmic volume probed with redshifts by an order of magnitude. 
%They have the potential to measure the growth of structure and \mneut~with unprecedented precision.
%eBOSS and DESI will probe unprecedented cosmic volumes with galaxy redshifts and have the potential to measure the growth of structure and \mneut~with extraordinary precision. 
%Galaxy clustering and thereby the growth of structure from RSD and \mneut ~can be measured with unprecedented precision. 
%The main obstacles for realizing the full statistical power are methodological and can be solved by robust treatment of systematics, accurate probabilistic inference, and higher order statistics.\\ \vspace{-3mm}

For instance, observational systematics such as fiber collisions will continue 
to impact galaxy clusteing analyses of eBOSS and DESI, which will utilize 
fiber-fed spectrographs. As described in~\Chap{fc}, due to the impact that fiber 
collisions have on small scales, much of the statistical gains from eBOSS and 
DESI will be {\em wasted} if they are not properly account for in analyses. 
In fact, in eBOSS and DESI the systematics will be more complicated with multiple 
classes of target objects and automated fiber positioning 
\citep{Cahn:2015_desifib, Dawson:2015aa}. But the methods from ~\Chap{fc} 
can be extended to both surveys.

\todo{
Furthermore, in~\Chap{abc}, we revealed deviations between the ABC posterior 
probability distribution and the standard Gaussian pseudo-likelihood approach to 
inference -- even in the narrower context of halo occupation modeling. Yet 
there have not been direct investigations on the impact of the standard 
assumptions on more general cosmological parameter constraints. With the 
increased statistical power of future surveys, quantifying the impact of these 
assumptions in our inference is critical for unbiased constraints. While
tractability of forward modeling the data has been an obstacle for adopting 
ABC, new models aimed at the next galaxy surveys, are making promising 
strides. 
}

Finally, as we describe in~\Chap{galenv}, observations of galaxies have 
firmly established a global view of galaxy properties out to $z{\sim}1$. 
As in \Chap{galhalo}, precise predictions of hierarchical growth of 
structure from $\Lambda$CDM can be used to constrain key elements of 
galaxy evolution in a data-driven and statistical fashion. The introduction 
of Integral Field Unit observations (\emph{e.g.} MaNGA) and larger 
galaxy samples (\emph{e.g.} DESI Bright Galaxy Survey) offer exciting 
opportunities to extend the works of \chapname s~\chapalt{galenv} and~\chapalt{galhalo}
and construct better models of the galaxy-halo connection.

Each aspect of my dissertation will be instrumental for exploiting the full 
potential of future surveys and making more precise measurements of the growth
rate of structure, the cosmological parameters, and thus tests of General Relativity 
and modified gravity scenarios. 
%Measuring this redshift-space distortion (RSD) allows us to infer the growth of structure, which we can then use to test GR and modified gravity scenarios. 
%doubly instrumental for extracting accurate and precise \mneut ~constraints from eBOSS and DESI.
Furthermore, galaxy clustering also provides a unique window to probe 
fundamental physics --- {\em i.e.} the total neutrino mass (\mneut). Neutrinos 
suppress the growth of structure below their free-streaming scale and leave 
imprints on the LSS. Extending the methods from my dissertation to future 
surveys will also allow tigher constraints to be placed on \mneut. A tighter 
upper limit on \mneut ~is essential to distinguish between the neutrino mass 
hierarchies and will provide an important input for particle physics theory 
beyond the Standard Model.

%Furthermore, accounting for systematics with improved modeling is essential for \mneut ~constraints. 
%Analysis beyond $k \sim 0.1h/\mathrm{Mpc}$ is {\em necessary} to distinguish the neutrino mass hierarchies by more than $1 \sigma$ \citep[][DESI \emph{FDR}]{font_neut}. 
%$B(\vec{k})$ is also critical for breaking the degeneracies between \mneut~and galaxy bias parameters. 
%{\bf Using the methodological improvements I will implement in my research program, I will jointly analyze $P(k)$ and $B(\vec{k})$ in eBOSS and DESI to directly infer \mneut.}


%Constraints exist from $P(k)$ analyses; however, the latest analyses do {\em not} directly infer \mneut \citep{beut_neut, gm_neut2015}.  Instead, they indirectly derive \mneut using growth rate of structure constraints.  They also do not properly treat systematics, which is especially alarming since imprints of \mneut ~and FCs on $P(k)$ are degenerate (Figure \ref{fig:fc_demo}). 

%Cosmology, which precisely predicts the dark matter evolution, provides a framework for answering {\em specific} and {\em tractable} questions in galaxy evolution.  In $\Lambda$CDM, structures form \emph{``hierarchically''} --- smaller ones form earlier and subsequently merge to form larger ones.  The galaxy population can be positioned in this framework and treated in a data-driven and statistical fashion to constrain key elements of their evolution \citep[\emph{e.g.}][]{wetzel2013}.  Our understanding of galaxies depends on combining galaxy properties derived from {\em heterogeneous} observations and methodologies. A thorough and direct comparison of these methodologies is critical to derive precise and coherent interpretations of galaxy evolution. The introduction of Integral Field Unit (IFU) data offers a key opportunity to extend past efforts~\citep[\emph{e.g.}][]{salim} and conduct a principled comparison. 

%The seminal MPA-JHU catalog\footnotemark[2] derived galaxy properties from optical spectra obtained from single-fibers~\citep{mpajhu_mass, mpajhu_sfr}.  With one fiber per galaxy, mainly the centers were observed.  IFU surveys like MaNGA provide 2D spectroscopic maps of {\em entire} galaxies~\citep{manga}.  Using MPA-JHU methodology, I propose to derive stellar masses and SFRs of all {\rm ${\sim}10,000$} MaNGA galaxies and compare them to their properties derived from single-fiber spectra in the NASA-Sloan Atlas\footnotemark[3] (NSA).  Furthermore, using multi-wavelength photometry in the NSA from GALEX, 2MASS, and WISE, I will also compare properties derived from spectral energy distribution (SED) fitting of photometry, which due to observational costs of precise spectroscopy, is the {\em dominant} means of deriving galaxy properties at higher redshifts. 

%The robust galaxy properties from IFU spectra will produce anchoring measurements to calibrate our understanding of galaxies.  Moreover, the comparison will provide an essential test bed for interpreting heterogenous observations and methodologies.

%Observations carry the imprints of how galaxies connect to their host dark matter halos.  Stellar masses of galaxies, for instance, form an empirical relation with the masses of their halos that has uniform scatter over $z< 1$~\citep[\emph{e.g.}][]{smhmr}.  This stellar-to-halo mass relation (SHMR) links halo growth to stellar mass growth and consequently to galaxy star formation history (SFH).  Observations also find correlation in SFRs among ``central'' galaxies and their neighbors~\citep{kauff_conform}.  This ``\emph{galactic conformity}'' has been regarded as the ``smoking gun'' of the link between the formation history of halos and galaxy star formation.%~\citep[][]{hearin_conformity}. 

%In \cite{cenque}, using a data-driven model that coupled observed trends in galaxy SFRs with hierarchical growth of structure, I measured the star formation quenching timescale.  Information encoded in observation can be revealed in this way to constrain key elements of galaxy evolution.  I propose to extend {\rm \cite{cenque}} models with observations of SHMR and galactic conformity, then use them to constrain galaxy SFHs and quantify the impact of host halo properties on star formation in galaxies.  With the difficulty that observations have in deriving detailed galaxy SFHs~\citep[\emph{e.g.}][]{dressler}, these constraints will be pivotal in revealing how galaxy form and evolve their star formation. 


%Fiber collisions (FCs), for instance, prevent surveys from collecting redshifts due to physical constraints on the focal plane. 
%Their impact on the galaxy power spectrum, $P(k)$, goes well beyond their angular scale and restricts analysis on scales below $k{\sim}0.1 h/\mathrm{Mpc}$.
%Analyses on such a restricted range produce growth of structure constraints with {\em over twice}\footnote{DESI \textit{Final Design Report} (FDR): \url{http://desi.lbl.gov/tdr/}} the fractional error of analyses to $k{=} 0.2 h/\mathrm{Mpc}$. 
%Much of the statistical gains from eBOSS and DESI will be {\em wasted} unless we better account for systematics. 
%
%For BOSS, I developed methods that successfully account for FCs and extend accurate $P(k)$ analysis to smaller scales than ever possible \citep[$k{\sim}0.3h/\mathrm{Mpc}$;][]{fibcol}.
%In eBOSS and DESI the systematics will be more complicated with multiple classes of target objects and automated fiber positioning \citep{desi_fib, eboss}. 
%But the \cite{fibcol} methods extend to both surveys. 
%Building on those methods, I propose to develop a framework for properly treating systematics in $P(k)$ multipole analyses for eBOSS and DESI.
%
%To realize the advantages from extending to smaller scales, we must also reliably model those scales. 
%Current $P(k)$ models are mainly derived from perturbation theory, which breaks down at small scales from non-linearities%~\citep{seljak_ptfail1}. % structure formation~\citep{seljak_ptfail1}. 
%Emulation using Gaussian Process interpolation has been excitingly successful at modeling the non-linear matter $P(k)$. %\citep[\emph{e.g.}][]{emulator}. %coyote2, 
%I propose to utilize Gaussian Process emulation to extend $P(k)$ models down to quasi non-linear scales. 
%Better treatment of systematics and modeling, which will unlock the constraining power from smaller scales with higher signal-to-noise, are critical for extracting the full statistical potential of eBOSS and DESI.

%The standard approach to parameter inference in LSS {\em assumes} that the likelihood function is Gaussian. While this is suitable for CMB and on large scales, with galaxies that formed through non-linear gravitational evolution, the likelihood {\em cannot} be Gaussian. Methods like {\em Approximate Bayesian Computation} (ABC) generate formal probabilistic constraints without deriving an explicit likelihood function~\citep[\emph{e.g.}][]{Weyant}.However, they are not yet widely implemented in LSS. 

%In an effort to adopt ABC for LSS, I implemented ABC in the halo occupation framework \citep{abc}. Even in the narrower framework, the ABC posterior probability distribution deviates from that of  the standard approach.Yet there are {\em no} direct investigation on the impact of the standard assumption on more general cosmological parameter constraints. 

%One obstacle for adopting ABC has been the accuracy and tractability in forward modeling the data. New models, aimed at eBOSS and DESI, are making great strides in these regards \citep[\emph{e.g.}][]{qpm, fastpm}. {\bf Combining ABC with these models, I will directly quantify the impact of the Gaussian likelihood assumption on parameter constraints from $P(k)$ multipole analyses in eBOSS and DESI. Quantifying the impact of assumptions in our inference is crucial for unbiased constraints, especially as uncertainties diminish with eBOSS and DESI.

%RSD analyses limited to the $P(k)$ multipoles are confronted with the degeneracy between the $P(k)$ amplitude, galaxy bias parameters, and the growth rate of structure. % ($\sigma_8$, $b$, and $f$). 
%The bispectrum, $B(\vec{k})$, the three-point function in Fourier space, {\em breaks this degeneracy}.
%The dependence on triangle configuration in $B(\vec{k})$ disentangles contributions from gravitational instability versus non-linear biasing of galaxies. 
%On small scales ($k {>} 0.1h/{\rm Mpc}$), $B(\vec{ k})$ has higher signal-to-noise than $P(k)$, which makes a combined analysis even more desirable \citep{sefusatti_bisp1}.%, sefusatti_bisp2}.
%
%Recent works have demonstrated the use of $B(\vec{k})$ in analyzing RSD \citep{gm_bisp2015, gm_bisp2016}. 
%They, however, do not properly treat systematics, which biases the analysis (see Figure \ref{fig:bisp}). 
%I am developing methods to account for FCs in $B(\vec{k})$ by extending the methods for $P(k)$. 
%Incorporating these methods, I will develop a framework that consistently treats systematics for the $P(k)$ multipole and $B(\vec{k})$ in a joint analysis. 
%Then I will measure the growth rate of structure using this joint analysis for eBOSS and DESI. 


%In this dissertation, we study the population of exoplanets using data from
%NASA's \kepler\ Mission and the re-purposed \KT\ Mission.
%We develop and apply novel techniques to discover previously unknown planets
%and planet candidates (\chapname s~\chapalt{ketu} and~\chapalt{peerless}).
%We present a robust probabilistic framework for making inferences about the
%population of exoplanets based on the noisy and incomplete catalogs derived
%from transit surveys (\chap{exopop}).
%The main contributions of this dissertation are methodological and each
%\chapname\ is accompanied by open source software implementing the methods.
%
%In the spirit of tool development and open source software, \Chap{emcee} is
%describes \project{emcee}, a general purpose Markov Chain Monte Carlo sampler
%that, since its release \citep{Foreman-Mackey:2013}, has become one of the
%most popular tools for probabilistic inference in astronomy.
%This method was originally proposed by \citet{Goodman:2010} and it was
%designed to sample problems efficiently with little tuning even when the
%parameter space is poorly conditioned.
%This feature is especially useful for problems in astronomy where the physical
%parameters often vary (and covary) over many orders of magnitude.
%The \project{emcee} implementation offers a small performance gain by deriving
%a parallelizable version of the original algorithm and a user-friendly and
%well documented Python interface.
%In practice, this method doesn't scale well to large numbers of dimensions
%($\gtrsim 50$) but it has been shown to work out-of-the-box on a large class
%of typical astronomy problems.
%
%In \Chap{exopop}, we derive a hierarchical method for inferring the population
%of exoplanets based on a catalog of planets with a non-trivial completeness
%function and large measurement uncertainties.
%This method builds on the importance sampling technique originally derived by
%\citet{Hogg:2010a} to make a clean histogram from noisy measurements.
%Applying this population inference method to a catalog of planet candidates
%transiting Sun-like stars \citep{Petigura:2013}, we make a prediction for the
%rate of Earth analogs.
%This prediction is substantially lower than earlier predictions based on the
%same catalog.
%We demonstrate that this discrepancy is caused by both the treatment of the
%observational uncertainties and the choice of extrapolation function.
%
%In Summer 2014, the \kepler\ spacecraft was re-purposed and it began taking
%data for the \KT\ Mission.
%The pointing accuracy in this mode is substantially degraded relative to the
%original Mission but, in \chap{ketu}, we demonstrate that these light curves
%can still be used to systematically search for transiting exoplanets.
%By building a flexible data-driven model for the systematic variability in the
%light curves of the stars and combining this with an approximate linear
%transit model, we derive a transit search algorithm where the systematics
%model is marginalized for every hypothesis.
%This enables the discovery of transit signals with amplitudes smaller than the
%pointing-induced variability.
%In \chap{ketu}, we announce the discovery of 36 planet candidates transiting
%33 stars.
%Of these candidates, 18 have been validated as bona fide planets and 6 have
%been identified as likely astrophysical false positives
%\citep{Crossfield:2015, Montet:2015, Armstrong:2015a}.
%
%Finally, in \chap{peerless}, we present a novel method for detecting the
%transits of planets with orbital periods longer than the baseline of
%observations.
%Existing transit search methods are blind to these long periods because it is
%technically difficult to distinguish a single transit from coincidental
%variability in light curves.
%This constraint is not acceptable for forthcoming surveys like \KT\ and \tess\
%where the observation baselines are shorter than the periods of the most
%important planets for studies of dynamics and habitability.
%We apply a supervised classification algorithm, implemented using a set of
%Random Forest classifiers trained on simulated transits, to predict the
%``class'' (\texttt{transit} or \texttt{no transit}) of every section of light
%curve.
%Using this method, we announce the discovery of a convincing single transit
%candidate with a radius of $\sim 2\,R_\mathrm{J}$.
%
%The ultimate goal of this research program is an improved understanding of the
%population of exoplanets at the currently uncharted extremes of parameter
%space, especially pushing to long periods.
%This dissertation represents a step in this direction but there are some
%conspicuously missing pieces in the methods presented in these pages.
%One major shortcoming is that neither \chap{ketu} or \chap{peerless} realized
%the dream of a fully automated search.
%In both projects, a final stage of manual vetting was required to reach the
%target precision.
%This is unacceptable if we want to make rigorous inferences of the population
%of planets because human components of a pipeline can't be stress-tested and
%characterized for consistency and performance.
%The main barrier to completely automated search is that we don't have an
%acceptable generative model for the signals that are mis-classified by the
%search algorithms and we can never be completely sure that any section of
%light curve \emph{does not have any transits}.
%This goal of fully automated transit discovery will become even more important
%as new datasets continue to roll in from \KT, \tess, and \plato.
%This should be a focus of large scale transit programs over the next years.

